{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Framing\n",
    "## 1-1. Supervised Learning\n",
    "- __Q. Suppose you want to develop a supervised machine learning model to predict whether a given email is \"spam\" or \"not spam.\" Which of the following statements are true? (주어진 이메일이 '스팸' 인지 '스팸이 아닌지' 예측하기 위해 지도 머신러닝 모델을 개발한다고 합시다. 다음 중 참인 내용은 무엇일까요?)__ <br>\n",
    "<br>\n",
    "    - Words in the subject header will make good labels. (제목 헤더의 단어는 라벨로 사용하기에 적절합니다.)\n",
    "        - Words in the subject header might make excellent features, but they won't make good labels. (제목 헤더의 단어는 특성으로 사용하기에는 적절하지만 라벨로 사용하기에는 적절하지 않습니다.)<br>\n",
    "    <br>\n",
    "    - __Emails not marked as \"spam\" or \"not spam\" are unlabeled examples. ('스팸' 또는 '스팸 아님'으로 표시되지 않은 이메일은 라벨이 없는 예입니다.)__\n",
    "        - Because our label consists of the values \"spam\" and \"not spam\", any email not yet marked as spam or not spam is an unlabeled example. (라벨이 '스팸' 및 '스팸 아님' 값으로 구성되므로 아직 스팸 또는 스팸 아님으로 표시되지 않은 이메일은 모두 라벨이 없는 예입니다.)<br>\n",
    "        <br>\n",
    "    - We'll use unlabeled examples to train the model. (모델을 학습시키는 데는 라벨이 없는 예를 사용합니다.)\n",
    "        - We'll use labeled examples to train the model. We can then run the trained model against unlabeled examples to infer whether the unlabeled email messages are spam or not spam. (모델을 학습시키는 데는 라벨이 있는 예를 사용합니다. 그런 다음 학습된 모델을 라벨이 없는 예에 적용하여 라벨이 없는 이메일 메시지가 스팸인지 스팸이 아닌지 추론할 수 있습니다.) <Br>\n",
    "    <br>\n",
    "    - __The labels applied to some examples might be untrustworthy. (일부 라벨은 신뢰할 수 없을 수도 있습니다.)__\n",
    "        - Definitely. The labels for this dataset probably come from email users who mark particular email messages as spam. Since very few users mark every suspicious email message as spam, we may have a hard time ever knowing whether an email is spam. Furthermore, some spammers or botnets could intentionally poison our model by providing faulty labels. \n",
    "(확실히 이 데이터 세트의 라벨은 특정 이메일 메시지를 스팸으로 표시하는 이메일 사용자로부터 가져온 것일 수도 있습니다. 의심스러운 모든 이메일 메시지를 스팸으로 표시하는 사용자는 아주 적기 때문에 특정 이메일이 스팸인지 확인하기 어려울 수도 있습니다. 또한 일부 스팸 발송자나 봇넷은 결함이 있는 라벨을 제공하여 모델을 고의적으로 손상시킬 수 있습니다.)\n",
    "\n",
    "\n",
    "## 1-2. Features and Labels\n",
    "- __Q. Suppose an online shoe store wants to create a supervised ML model that will provide personalized shoe recommendations to users. That is, the model will recommend certain pairs of shoes to Marty and different pairs of shoes to Janet. Which of the following statements are true? (한 온라인 신발가게에서 사용자에게 맞춤형 신발을 추천하는 지도 ML 모델을 만들려고 합니다. 즉 이 모델에서는 철수에게 특정 신발을 추천하고 영희에게는 다른 신발을 추천합니다. 다음 중 참인 내용은 무엇일까요?)__ <br>\n",
    "    <br>\n",
    "    - __User clicks on a shoe's description is a useful label. (사용자의 신발 설명 클릭수는 유용한 라벨입니다.)__\n",
    "        - 사용자는 마음에 드는 신발의 설명만 읽으려 할 것입니다. 따라서 사용자 클릭수는 좋은 학습 라벨로 사용할 수 있는 관찰 가능하고 수량화 가능한 측정항목입니다. <br>\n",
    "    <br>\n",
    "    - The shoes that a user adores is a useful label. (사용자가 아주 좋아하는 신발은 유용한 라벨입니다.)\n",
    "        - Adoration is not an observable, quantifiable metric. The best we can do is search for observable proxy metrics for adoration. (좋아함은 관찰 가능하고 수량화 가능한 측정항목이 아닙니다. 최선의 방법은 좋아함 대신 관찰 가능한 대리 측정항목을 검색하는 것입니다.) <br>\n",
    "    <br>\n",
    "    - __Shoe size is a useful feature. (신발 크기는 유용한 특성입니다.)__\n",
    "        - Shoe size is a quantifiable signal that likely has a strong impact on whether the user will like the recommended shoes. For example, if Marty wears size 9, the model shouldn't recommend size 7 shoes. (신발 크기는 추천된 신발이 사용자의 마음에 들 것인지 쉽게 확인할 수 있는 수량화 가능한 신호입니다. 예를 들어 철수의 신발 크기가 270cm인 경우 모델은 260cm 신발을 추천하면 안 됩니다.) <br>\n",
    "    <br>\n",
    "    - Shoe beauty is a useful feature. (신발의 아름다움은 유용한 특성입니다.)\n",
    "        - Adoration is not an observable, quantifiable metric. The best we can do is search for observable proxy metrics for adoration. (구체적이고 수량화 가능한 특성이 좋은 특성입니다. 아름다움은 너무 모호한 개념이어서 특성으로 사용하기에 적절하지 않습니다. 아름다움은 스타일, 색상 등 구체적인 특성이 혼합된 특성일 것입니다. 스타일과 색상은 각각 아름다움보다 더 유용한 특성입니다.)\n",
    "\n",
    "\n",
    "# 2. Descending into ML\n",
    "## 2-1. MSE(Mean Squared Error)\n",
    "<img src = \"https://photos-3.dropbox.com/t/2/AADC5isN2ZG5rgwXWBewx4Uz9oistbyFmpvgc2t0ucMIOg/12/848258672/png/32x32/1/_/1/2/Screenshot%202018-06-22%2017.01.01.png/EPDS_JsJGIYBIAIoAg/98Zww2z6s3RMPY6YIhJIz-knC_UvC7JLiB2rsErUawM?preserve_transparency=1&size=2048x1536&size_mode=3\">\n",
    "- __Q. Which of the two data sets shown in the preceding plots has the higher Mean Squared Error (MSE)? (위 도표에 표시된 두 개의 데이터 세트 중 MSE(평균 제곱 오차)가 더 높은 것은 무엇인가요?)__ <Br>\n",
    "    <Br>\n",
    "    - __The dataset on the right.__<Br>\n",
    "        - The eight examples on the line incur a total loss of 0. However, although only two points lay off the line, both of those points are twice as far off the line as the outlier points in the left figure. Squared loss amplifies those differences, so an offset of two incurs a loss four times greater than an offset of one. (선에 위치한 8개의 점에 발생하는 총 손실은 0입니다. 그러나, 선에서 벗어난 점은 2개밖에 안 되지만, 2개의 점 모두 선에서 벗어난 정도가 왼쪽 그림의 이상점에 비해 2배 더 큽니다. 제곱 손실값의 경우 이러한 차이가 증폭되므로 오프셋 2의 경우 오프셋 1보다 4배 더 큰 손실이 발생합니다.)\n",
    "<img src =\"https://photos-2.dropbox.com/t/2/AACamb5udHDAjDOiF2Bdd5Tn7IMFf5Tv1pbX3JT5rfN5XQ/12/848258672/png/32x32/1/_/1/2/Screenshot%202018-06-22%2016.05.47.png/EPDS_JsJGIQBIAIoAg/Q4PuXYXPw6GxZLT7zW0cDM3bIeWgJVHPt5A87P6fjfE?preserve_transparency=1&size=2048x1536&size_mode=3\">\n",
    "        <br>\n",
    "    - The dataset on the left.<Br>\n",
    "        - The six examples on the line incur a total loss of 0. The four examples not on the line are not very far off the line, so even squaring their offset still yields a low value: (선에 위치한 6개의 점에 발생하는 총 손실은 0입니다. 선에서 벗어나 있는 4개의 점은 벗어난 정도가 그다지 크지 않으므로 오프셋을 제곱해도 값이 여전히 낮습니다.)\n",
    "    <img src=\"https://photos-6.dropbox.com/t/2/AADb9TtFV2a92FAiGdixE50jxvzCDQ8rQ1xPlr_FmJO0Rw/12/848258672/png/32x32/1/_/1/2/Screenshot%202018-06-22%2016.08.31.png/EPDS_JsJGIUBIAIoAg/1JG8Kl6kl8FOwOjF_6HHFUbqGPqrP5YSkm6OL1BQuoA?preserve_transparency=1&size=2048x1536&size_mode=3\">\n",
    "\n",
    "# 3. Reducing Loss\n",
    "## 3-1. Batch Size\n",
    "- __Q. When performing gradient descent on a large data set, which of the following batch sizes will likely be more efficient? (다음 중 대량의 데이터 세트에서 경사하강법을 수행할 때 더 효율적인 배치 크기는 어느 것일까요?)__ <Br>\n",
    "    <Br>\n",
    "    - __A small batch or even a batch of one example (SGD).__\n",
    "        -  Computing the gradient from a full batch is inefficient. That is, the gradient can usually be computed far more efficiently (and just as accurately) from a smaller batch than from a vastly bigger full batch. (놀랍게도 소규모 배치 또는 예가 하나뿐인 배치에서 경사하강법을 수행하는 것이 일반적으로 전체 배치보다 훨씬 효율적입니다. 예시 한 개의 기울기를 찾는 것이 예시 수백만 개의 기울기를 찾는 것보다 훨씬 간단한 법이니까요. 좋은 대표 샘플을 확보하기 위해 알고리즘은 반복마다 다른 소규모 배치 또는 예가 하나뿐인 배치를 무작위로 확보합니다.) <Br>\n",
    "            <Br>\n",
    "    - The full batch.\n",
    "        - Amazingly enough, performing gradient descent on a small batch or even a batch of one example is usually more efficient than the full batch. After all, finding the gradient of one example is far cheaper than finding the gradient of millions of examples. To ensure a good representative sample, the algorithm scoops up another random small batch (or batch of one) on every iteration.\n",
    " (전체 배치에서 기울기를 계산하는 것은 비효율적입니다. 즉, 일반적으로 훨씬 더 큰 전체 배치에서보다 작은 배치에서 정확도는 비슷하면서 훨씬 효율적으로 기울기를 계산할 수 있습니다.)\n",
    "                \n",
    "# 4. TensorFlow\n",
    "## 4-1. tf.estimator API\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up a linear classifier.\n",
    "classifier = tf.estimator.LinearClassifier()\n",
    "\n",
    "# Train the model on some example data.\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "\n",
    "# Use it to predict.\n",
    "predictions = classifier.predict(input_fn=predict_input_fn)\n",
    "```\n",
    "\n",
    "## 4-2. TensorFlow toolkit.\n",
    "- TensorFlow toolkit hierarchy.\n",
    "    - The following table summarizes the purposes of the different layers:\n",
    "    \n",
    "||TensorFlow toolkit hierarchy.|description||\n",
    "|------|------|------|------|------|\n",
    "||tf.estimator|High-level, OOP API(높은 수준의 OOP API).|\n",
    "||tf.layers/tf.losses/tf.metrics|Libraries for common model components(일반 모델 구성요소용 라이브러리).|\n",
    "||tensorflow|Lower-level APIs(낮은 수준의 API).|\n",
    "\n",
    "- TensorFlow consists of the following two components: (텐서플로우는 다음 두 요소로 구성됩니다.)<br>\n",
    "<br>\n",
    "    - [a graph protocol buffer](https://www.tensorflow.org/extend/tool_developers/#protocol_buffers)\n",
    "    - a runtime that executes the (distributed) graph (분산된 그래프를 실행하는 런타임) <br>\n",
    "    <br>\n",
    "    \n",
    "These two components are analogous to Python code and the Python interpreter. Just as the Python interpreter is implemented on multiple hardware platforms to run Python code, TensorFlow can run the graph on multiple hardware platforms, including CPU, GPU, and TPU (이 두 구성요소는 자바 컴파일러 및 JVM과 유사합니다. JVM이 여러 하드웨어 플랫폼에서 구현되는 것과 마찬가지로 텐서플로우도 여러 CPU와 GPU에서 구현됩니다).\n",
    "\n",
    "Which API(s) should you use? You should use the highest level of abstraction that solves the problem. The higher levels of abstraction are easier to use, but are also (by design) less flexible. We recommend you start with the highest-level API first and get everything working. If you need additional flexibility for some special modeling concerns, move one level lower. Note that each level is built using the APIs in lower levels, so dropping down the hierarchy should be reasonably straightforward (어느 API를 사용해야 하나요? 문제를 해결하는 최고 수준의 추상화를 사용해야 합니다. 추상화 수준이 높을수록 더 사용하기 쉽지만 (설계상) 유연성이 떨어집니다. 먼저 최고 수준의 API로 시작하여 모든 작업을 실행하는 것이 좋습니다. 특별한 모델링 문제를 해결하기 위해 더 유연한 추상화가 필요하면 한 수준 아래로 이동합니다. 각 수준은 낮은 수준의 API를 사용하여 제작되므로 계층구조를 낮추는 것이 합리적입니다).\n",
    "\n",
    "## 4-3. tf.estimator API\n",
    "We'll use tf.estimator for the majority of exercises in Machine Learning Crash Course. Everything you'll do in the exercises could have been done in lower-level (raw) TensorFlow, but using tf.estimator dramatically lowers the number of lines of code (머신러닝 단기집중과정 내 대부분의 실습에서 tf.estimator를 사용합니다. 낮은 수준의(원시) 텐서플로우를 사용해도 실습의 모든 작업을 실행할 수 있지만 tf.estimator를 사용하면 코드 행 수가 크게 줄어듭니다).\n",
    "\n",
    "tf.estimator is compatible with the scikit-learn API. Scikit-learn is an extremely popular open-source ML library in Python, with over 100k users, including many at Google (tf.estimator는 scikit-learn API와 호환됩니다. scikit-learn은 Python의 매우 인기 있는 오픈소스 ML 라이브러리로, Google 직원을 비롯하여 100,000명이 넘는 사람들이 이용하고 있습니다).\n",
    "\n",
    "Very broadly speaking, here's the pseudocode for a linear classification program implemented in tf.estimator: (tf.estimator로 구현된 선형 회귀 프로그램의 형식은 대체로 다음과 같습니다).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up a linear classifier.\n",
    "classifier = tf.estimator.LinearClassifier()\n",
    "\n",
    "# Train the model on some example data.\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "\n",
    "# Use it to predict.\n",
    "predictions = classifier.predict(input_fn=predict_input_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
