{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이산형 확률분포 & 연속형 확률분포\n",
    "- 이산형(discrete) : 정수로 딱 떨어져서 셀 수 있는 경우\n",
    "- 연속형(continuous) : 정수로 딱 떨어지지 않아 셀 수 없는 경우\n",
    "    - 일정 __구간__을 설정하고 '이 구간 안에 몇 명이 있느냐?'라는 방식으로 확률을 측정한다\n",
    "- 확률변수도 확률분포와 마찬가지로 데이터의 특성에 따라, 크게 이산확률변수와 연속확률변수로 나뉜다\n",
    "- 참고 : http://math7.tistory.com/20\n",
    "\n",
    "## 2. 지수분포\n",
    "- 연속확률분포 중에서, 일정구간에서 일어날 확률이 균등할 때는 균등분포를 사용한다.\n",
    "- 그리고 평균에서 가까울수록 일어날 확률이 커지고, 평균에서 멀어질수록 확률이 작아질 때는 정규분포를 사용한다.\n",
    "- 그런데 이러한 경우 외에 __시간이 지날수록 (발생)확률이 점점 작아지는 경우__도 있는데, 이럴 때 사용하는 분포가 지수분포다.\n",
    "- 연속성 확률분포들은 그래프의 면적으로 확률을 구한다고 했는데[[참고](http://math7.tistory.com/42)]\n",
    "- 지수분포는 시간이 지날수록 확률이 작아지기에 그래프가 점점 기운다는 특징이 있다.\n",
    "\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/26477546543A42A80C\">\n",
    "<center><strong>[시간이 지날수록 p(x) 감소]</center>\n",
    "\n",
    "\n",
    "## 3. 정규화 & 표준화\n",
    "- __정규화(Normalization)__<br>\n",
    "<br>\n",
    "    - x - min(x) / max(x) - min(x)\n",
    "    - 변수를 비율로 바꾸는 것\n",
    "        - 확률을 0~1 사이로 바꿔주는 작업\n",
    "        - 의미가 없음(애초에 확률은 0~1이니까) <br>\n",
    "        <br>\n",
    "- __표준화(Standardzaition, Scale)__<br>\n",
    "<br>\n",
    "    - 평균을 0으로 맞춰주고, 분산을 1로 맞춰주는 것\n",
    "    - 서로 다른 집합의 변수 단위를 맞추는 작업\n",
    "    - obj : 단위 통일<br>\n",
    "    <br>\n",
    "- Regularization<br>\n",
    "<br>\n",
    "    - 알고리즘에서 쓰이는 언어\n",
    "\n",
    "## 4. 변환(재표현)의 사다리\n",
    "- __[변환의 사다리]__(https://m.blog.naver.com/PostView.nhn?blogId=chcher&logNo=70137838100&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F)<br>\n",
    "<br>\n",
    "    - 편향된 분포를 띠는 데이터들의 형태를 정규분포 형태로 바꾸는 작업?<br>\n",
    "    <br>\n",
    "    - 핵심은 선형을 만드는 것\n",
    "        - 변수 하나가 \n",
    "        - 사다리는 두 변수 간의 선형을 만들어주는 것\n",
    "        - 선형 모형(관계)를 가지게 만드는 것<br>\n",
    "        <br>\n",
    "    - 기계학습은 비선형, 통계적 기법은 선형\n",
    "\n",
    "## 5. 모수적 방법론 & 비모수적 방법론\n",
    "### 5-1.  모수적 방법론(Parametric Method)\n",
    "- 전통적인 통계적 접근법<br>\n",
    "<br>\n",
    "- 분포와 변수를 미리 지정\n",
    "    - 정규, 지수, 푸아송, 이항, 다항 분포\n",
    "    - y = β0 + β1x1 + β2x2 + e ?\n",
    "        - __구해야할 parameter를 고정하고 시작__<br>\n",
    "        <br>\n",
    "- __제약이 많다.__\n",
    "    - __구해야할 게 정해져 있으니까__\n",
    "    - __(어찌됐든)이 결과값을 내야한다.__\n",
    "    - 그런데 기계학습은 그게 아니다!\n",
    "\n",
    "### 5-2. 비모수적 방법론 (Non-Parametric Method)\n",
    "- 기계학습에 쓰이는 방법론<br>\n",
    "<br>\n",
    "- ML은 분포를 보지 않고 무조건 데이터를 때려 박아도 예측을 잘 해준다.<br>\n",
    "<br>\n",
    "- __변수 중요도는 나오지만, 변수(동시)비교가 안 된다.__\n",
    "    - 횡단분석이 안 된다.\n",
    "    - __성능은 (모수적 방법보다)더 좋게 나오지만 해석하기 어렵다.__<br>\n",
    "    <br>\n",
    "- ML이라고 모든 걸 해결해줄 수 없다.\n",
    "    - 해석이 안 되니까\n",
    "\n",
    "### 5-3. 정리 요약\n",
    "- __모수__ : __구해야 할 parameter를 고정__시켜놓는 것\n",
    "- __비모수__ : 구해야 할 parameter를 __고정시키지 않는 것__<br>\n",
    "<br>\n",
    "- (회귀식)표현의 차이\n",
    "    - 관점의 차이\n",
    "        - 통계 : y = β0 + β1x1 + β2x2 + e ?\n",
    "        - 머신러닝 : y = wx + b<br>\n",
    "        <br>\n",
    "    - 목표의 차이\n",
    "        - 통계\n",
    "            - 최소제곱법 : 점과 회귀선의 거리가 최소가 되는 기울기와 절편을 구하는 것\n",
    "        - ML\n",
    "            - Gradient Descent : MSE<br>\n",
    "    <br>\n",
    "- 참고 : https://brunch.co.kr/@seoungbumkim/7\n",
    "\n",
    "## 6. MSE (Mean Squared Error)\n",
    "- 오차 제곱합의 평균\n",
    "    - Error : (모델)예측값 <-> (현실)관측값의 차이<br>\n",
    "    <br>\n",
    "- 여러 번 돌려서 나온 Error들을 Mean한 것이 MSE<br>\n",
    "<br>\n",
    "- Cross Validation Error: MSE 작업을 여러 번 반복한 것을 평균낸 것\n",
    "\n",
    "\n",
    "## 7. 수집에서의 적합성, 분석에서의 타당성 \n",
    "- __적합성__<br>\n",
    "<br>\n",
    "    - 해당 변수가 적합하냐<br>\n",
    "    <br>\n",
    "    - __변수를 모을 때, 특정 목표에 따라 모으는데 그 변수가 이 목표에 적합하냐__<br>\n",
    "    <br>\n",
    "    - 여론 조사도 적합성이 중요하다.\n",
    "        - 이중 부정\n",
    "        - 설문조사 도구가 적합하냐?<br>\n",
    "        <br>\n",
    "- __타당성 (C.V)__<br>\n",
    "<br>\n",
    "    - __(변수가 적합 했는데) 반복해도 비슷하게 나오는가?__<br>\n",
    "    <br>\n",
    "    - 통계는 무조건 랜덤이다.<br>\n",
    "    <br>\n",
    "    - __새로운 데이터가 들어왔을 때, (모형에 따라서)비슷한 결과값이 나와야 한다.__<br>\n",
    "    <br>\n",
    "    - 그래야지 잘 추정했다고 말할 수 있다.\n",
    "        - overfitting이 안 된 것.<br>\n",
    "        <br>\n",
    "    - __어떤 데이터가 와서 반복 측정해도 비슷하게 나오느냐?__\n",
    "\n",
    "\n",
    "## 8. 결국 중요한 건 '전처리'와 'EDA'\n",
    "- 결국 중요한 건, __전처리__와 __EDA__<br>\n",
    "<br>\n",
    "    - (무얼 분석하든) __핵심적인 변수 '1~2개'__ 찾으면 끝난 거다.<br>\n",
    "    <br>\n",
    "        - 그렇게 하려면, __수집 단계에서 '적합성'__이 맞아야 한다.<br>\n",
    "        <br>\n",
    "        - (만약에 들어온 데이터 자체가 아니라면) 찾으려 해도 못 찾는 것이다.<br>\n",
    "        <br>\n",
    "        - 알고리즘 잘 쓰는 게 이기는 게 아니라 __잘 수집하고, 잘 처리하는 사람이 이기는 것__이다.<br>\n",
    "        <br>\n",
    "        - 결국 데이터를 잘 수집한다는 건, 그만큼 아이디어가 좋다는 것이다.\n",
    "            - 그만큼 필드에 대한 이해가 높다는 것이다.\n",
    "            - 통계는 결국 도구이다.<br>\n",
    "            <br>\n",
    "- 가장 중요한 건 __필드에 대한 이해__\n",
    "\n",
    "### 9. 기타\n",
    "#### 9-1. 데이터를 만지는 곳은 다 Red Ocean\n",
    "- 레드오션일 수록 데이터에 집중한다.\n",
    "    - 비용 지출을 줄이기 위해\n",
    "\n",
    "#### 9-2. 분석 포트폴리오\n",
    "- 캐글\n",
    "- [문화관광부 빅데이터](http://www.tourbigdata.kr/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
