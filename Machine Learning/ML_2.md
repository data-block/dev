
# 1. review
- - -
- - -

# 2-1. Parametric Approach & Hypothesis & Capacity
- - -

### 종전의 프로그래밍과 머신러닝의 차이점
- 전문가 vs. 데이터

- - -

### obj : x (data 기반)
- 목표는 x를 찾는 것 (데이터 기반) 
- 모든 함수가 사는 공간에서 우리가 가지고 있는 데이터를 가장 잘 설명하는 데이터를 찾는다고 가정해보자.
> - 이런 접근 방식에는 문제가 있다. 범위가 너무 넓다. 거기에서 하나를 뽑는 건 어렵다.
- (우리가 하고 싶은 건) 우리 데이터를 가장 잘 설명하는 함수를 제한된 범위 내에서 찾자!
> - 모든 1차 함수가 사는 공간에서 우리가 가진 데이터를 가장 잘 설명하는 함수를 찾는 건 그렇게 어려운 일이 아니다.
> - 모든 데이터에 대해서 loss에 계산하고 그걸 다 더해서 cost를 계산하고 그걸 가장 줄여주는 w와 b를 찾자
- parametric approach (모수적 접근방법)
> - 함수를 찾는 문제 -> parameter를 찾는 문제로 바뀜
> - parmeter = w, b
> - 함수를 찾는 문제에서 변수를 찾는 문제로 축소되었다.
- non-parametric approach
> - decision tree
- 데이터 10개를 가장 잘 설명하는 함수를 찾았다.
> - 어디서? 1차 함수가 사는 공간 안에서 데이터를 가장 잘 설명하는 함수를 찾았다. 
> - 얘는 우리가 원하는 관계를 설명하기에는 너무 단순하다.
- 관심 대상의 확장
> - (1차 함수를 포함하는)모든 2차 함수가 사는 공간으로 우리의 관심 대상을 확장시키자
> - 모든 1차 함수가 사는 공간에서 우리가 원하는 함수를 찾는 게 아니라, 확장된 공간에서 우리가 가진 데이터를 가장 잘 설명하는 함수를 찾아보자!

### hypothesis (가설)
> - 우리가 x와 y는 y=wx+b 1차 함수의 관계로 표현이 될 수 있다고 가설을 세운 것
> - 우리가 세운 가설 중에서, 가장 설명력이 뛰어난 w와 b를 찾자는 것!
> - hypothesis 설정하면 얻는 장점
>> - 선택할 수 있는 함수의 범위를 크게 줄여준다.
>> - 모든 함수가 사는 공간에서 뽑는 게 아니라 제한된 영역에서 사는 공간에서 뽑으면 되기에 문제가 훨씬 쉬워진다.
>> - 함수를 찾는 문제가 -> w, b를 찾는 문제로 바뀜(모수적 접근 방법, parametric approach)

### Capacity
- hypothesis 1 : 1차 함수
> - 1차 함수는 Capacity가 부족하다.
>> - 1차 함수는 우리가 가진 데이터를 표현할 수 있는 능력이 없다.

- hypothesis 2 : 2차 함수
> - Capacity가 조금 더 높은 2차 함수를 선택해보자.
>> - (2차 함수를 선택해서)우리가 가진 데이터를 사용해서 러닝 알고리즘을 통해서 가장 데이터를 잘 설명하는 w와 b를 찾자
>> - 우리가 가진 데이터의 비선형성을 더 잘 설명한다.
>> - 추세가 일치하는 경향성을 갖다가 우리는 선형성(linearity - 직선과 비슷한 양상)라고 부른다. 
>> - 데이터를 보면 선형적으로 움직이지 않고, 비선형적으로 움직인다.
>> - 그래서 비선형적인 관계를 모델링하기 위해서 2차 함수를 사용한다.

- hypothesis 3 : 3차 함수
> - 3차 함수로 우리가 가진 데이터를 모델링 해보자!
> - 모델이 더 좋아질까?
> - 아래의 cost와 연관됨.
>> - 판단 기준이 cost였다면, 2차 함수보다 3차 함수가 더 나은 모델이라고 정량적으로 얘기할 수 있다.
- cost
> - 모든 데이터의 loss를 다 더해서 계산한 값이 cost
> - loss는 예측값과 실제값의 차이를 계산해서 제곱한 값
>> - loss = (예측 - 실제)^2
> - 함수가 늘어나면 cost가 줄어들게 된다.
>> - 통계학에서는 r-squared라고 함.
>> - 변수를 추가하면 추가할 수록 r-suqared가 낮아진다.
> - 판단 기준이 cost였다면, 2차 함수보다 3차 함수가 더 나은 모델이라고 정량적으로 얘기할 수 있다.

- 질문: 차수가 하나 늘어날 수록 curve가 꺾이는 것인가?
> - 꼭 그렇지 않다.

### overfitting과 underfitting
- overfitting (과적합)
> - hypothesis 9 : 9차 함수
>> - 9차 함수는 cost가 0이다.
>> - cost를 모델의 판단 기준으로 삼기로 했다면, 9차 함수는 좋은 모델이다.
>> - 상식적으로 생각한다면, 좋은 모델이 아니다. 왜?
>>> - 새로운 data가 들어오면, 말도 안 되는 수치가 나온다.
>> - __새로운 데이터에 대한 예측력이 떨어진다.__
>>> - __우리가 가지고 있는 데이터로 모형(machine)을 만들어서, 새로운 데이터가 들어왔을 때 잘 예측하는 게 목표인데...__
>>> - 우리가 가지고 있는 데이터는 잘 설명하는데(cost=0이니까), 새로운 데이터에 대한 설명력이 감소한다.
>>> - 가지고 있는 데이터로 평가해봤자 아무짝에도 쓸모가 없다
>>>> - __overfitting(과적합)__: 과하게 맞추다.

- Underfitting (오적합)
> - 1차 함수는 너무 단순해서, 우리가 가지고 있는 데이터조차 설명을 잘하지 못한다.

- 모든 건 다 Capacity 때문에 발생하는 것이다.
> - 1차 함수는 Capacity가 너무 낮다, 9차 함수는 Capacity가 너무 높다.
>> - 1차 함수는 우리가 가지고 있는 데이터 조차 너무 표현하기 어렵고, 9차 함수는 가지고 있는 데이터에 너무 잘 맞춰져 있다.
> - 우리는 가장 적절한 Capacity를 찾는 모델이 필요하다.
> - ML을 할 때, 일반적으로 underfitting은 잘 안 일어난다. 왜냐면 우리가 사용하는 모델들이 기본적으로 Capacity가 높기 때문에.
> - overfitting이 정말 많이 일어난다. 극도로 경계해야하는 문제.
>> - ML system을 design할 때, 가장 신경써서 해결해야할 문제가 overfitting이다.
>>> - ex) 금융 : 삼성전자 주식 (2010~2017년) 데이터로 모델을 만들어서 내일의 주가를 예측하면 망한다.
>>> - 금융에서는 __백테스팅__을 한다. 우리가 배우는 개념을 금융에서는 백테스팅이라고 부른다.

- overfitting 문제를 해결해야 한다.
> - 해결 방안 1
>> - 데이터를 많이 모은다. Capacity 높은 모델에 big data를 때려 넣는다.
- __under / over fitting은 상대적이다.__
> - 1,000개의 데이터에 99,000개를 모아서 넣는다면, 1000개의 데이터에서 관측하지 못했던 새로운 구조가 드러난다. 
>> - 그러면 1, 3, 9차 함수를 통해서 우리가 가진 데이터를 모델링 해보자!
>>> - 1, 3, 9차 함수 모두 우리가 가진 데이터의 구조를 잘 표현하지 못했다. (underfitting)
- 언제 overfitting / underfitting 비교할 수 있는 정량적인 양을 [Bayes Error](http://newsight.tistory.com/127)라고 한다.

- underfitting의 문제
> - 데이터를 빼야할까?
>> - 그건 좋은 방법이 아니다.
> - 구조를 더 잘 모델링하기 위해서 (무지막지하게) 차수를 높이면 된다?
>> - 근데 그것도 좋은 방법이 아니다.
>> - (0.01)^100 = 1e-200 (10에 - 200승, 10에 200승 분의 1)
> - overflow와 underflow 문제
>> - underflow : 다 0이라고 계산하는 문제
>> - overflow : 너무 큰 애들은 
>>> 예를 들어서 컴퓨터가 표현할 수 있는 수가 -10000 ~ 10000까지 인데, 10001을 입력하면 컴퓨터는 10001이 아니라 -10000으로 인식하는 문제가 발생
> - 지금의 경우는 underflow의 문제가 발생할 수 있다.
> - 그래서 차수를 무지막지하게 늘려버리는 건 별로 좋은 아이디어가 아니다.
>> - 좋은 컴퓨터가 있으면 문제가 없을텐데...
> - 단순히 차수를 높여가는 모델은 한계가 있을 수 밖에 없다.
> - 그래서 우리는 신경망 모델이나 의사결정나무에 기반한 모델을 사용한다.

- 신경망 / 의사결정나무 / 랜덤포레스트 (33:30)
> - 다층퍼셉트론 : ?
> - 의사결정나무 : 
> - 랜덤포레스트: 의사결정나무 수천 개를 평균해서 만든 것
> - 현업에서 사용한다면 랜덤포레스트가 좋다.
>> - 다들 9차 함수보다 낫다,
> - 그래서 우리는 Capacity가 높은 모델군들이 필요하다.
>> - 우리가 더 많은 모델을 배우고자 하는 이유

- 데이터가 충분히 많아지면, 숨겨졌던 구조가 더 잘 드러나게 된다.
> - 데이터의 구조가 복잡하고, 데이터가 많기만 하면 뭐하나?
>> - 그 구조를 정확하게 모델링할 수 있는, 정확하게 표현할 수 있는, 그 능력(Capacity)을 가진 모델들이 필요한 것
>> - 그게 흔히 deeplearning이 해주는 일이다.
>> - 딥러닝 모델의 Capacity는 어마어마하게 크다.
>> - 그래서 두 개가 만났을 때, 좋은 성능을 보여준다.

- 데이터가 (상대적으로) 적은데, 모델 Capacity가 (상대적으로) 작다. 그러면 underfitting의 문제가 일어난다.
> - 근데 실제로는 잘 일어나지 않는다.

- 이런 문제가 더 중요하다.
- (상대적으로) 데이터가 충분히 많지 않음에도 불구하고, 모델 Capacity가 너무 커버리면 overfitting의 문제가 발생한다. 
> - __데이터의 수, Capacity는 상대적이다__

### __overfitting의 문제를 해결하려면,__
>> - 1) __데이터를 무지하게 많이 모으면 된다.__
>> - 2) (더 모을 수 없는 상황이라면) __모델 Capacity를 제한__하면 된다.
>>> - 더 작은 차수를 쓰도록 hypothesis를 바꿔준다.
>> - 3) (그게 아니라면) __9차 함수를 쓰기는 쓰되, 러닝 알고리즘을 바꿔준다.__
>>> - 학습하는 과정에서 우리가 가진 데이터에 너무 잘 맞지 않게끔 패널티를 줄 수 있다.
>>> - __regularization(정규화, 제약화)__

- overfitting 해결 방법 한 줄 요약
> - 데이터를 만들거나 더 작은 차수의 모델을 쓰거나 함수의 차수를 그대로 쓰되 러닝 알고리즘을 변경

- 질문
> - cost function
>> - cost function 값을 가지고 아무 것도 판단할 수 없다.
>> - 판단할 수 있는 유일한 건 우리가 선택한 모델이 우리가 가진 데이터를 얼마나 잘 설명했는가?
>> - __어떤 모델이 좋고 나쁜지를 가리기 위해서 새로운 선택 기준이 필요하다.__
>> - 그걸 cost라 안 하고 앞으로 train이라고 한다.

- - -
- - -

# 2-2. Cross - Validation

### ML에 대한 이야기
> - 통계학회, 텐서플로우, 팔로우
> - 1~4단원 차례대로 읽고, 7단원은 반드시 읽어라
>> - Model Assessment and Selection
>> - 우리가 선택한 모델들을 어떻게 평가하고 선택할 것인지
>>> - Bias, Variance, Bayesian Approach
>>> - __Cross-Validation(교차검증)__


## [실습 1](http://localhost:8888/notebooks/Downloads/dev/study/R_ML/Class%202/ML_2_script.ipynb)
### Object
- Electric Data
- 빌딩의 전력량을 예측하는 모델

### Hypothesis
- surface_area를 input으로 쓰면 electricity를 예측하는데 도움이 될 것이다.
> - 이걸 가설이라고 얘기하는 것인지? 그렇다면 1차~n차 함수도 가설로 얘기하는지?
> - input: surface_area (가설)

### EDA 
- feature : surface_area, electricity
- 일단 눈으로 확인하자!
> - plot함수


### Modeling
- lm(linear mode, 선형모델)
> - 우리가 input으로 넣어준 걸 선형 결합을 해서 output으로 표현을 하겠다.
> - 요걸 계산하겠다?
> - output : electricity
> - input : surface_area

```R
lm(formula = output ~ input, data=data)
```


### Model_1

```R
Model_1 = (surface_area * -0.7539) + 729.4538 = 612.22235
```

- 우리가 뭘한 것이냐, 슷자들을 추정한 것이다. 실제 값은 아니다.
- 실제 값이라는 보장은 없고, 우리가 가진 768개의 데이터를 사용해서 추정해낸 추정치
- 통계학에서는 저렇게 추정된 parameter들의 통계적 성질이 어떻게 될 것인가 연구한다.

- 예를 들어, 추정된 값이 -0.75인데 표준편차가 1이면, 표준편차까지 치면 얼마인가?
> - 0.7539 >= 1 >= 0.7539 안에 참 값이 들어갈 것이다.
> - 68%
>> - Adjusted R-squared:  0.4324 
>> - p-value: < 2.2e-16

```R
plot(Elec\$surface_area, Elec$electricity, cex = 3, pch = 19)
abline(Model_1, lwd = 10, col= "red")
```

![image.png](attachment:image.png)

- 우리가 방금 계산한 모델은 빨간선
> - 우리가 방금 계산한 모델이 경향성을 잘 설명한다고 느껴지는가?
> - 아니다.
> - 차수를 높이거나 다른 모델을 써야한다.

- 1차 함수의 cost 구하기
> - 모델에다가 데이터 넣어서 예측값 구하고, 실제 y값이랑 예측한 y값이랑 빼서 (더하거나) 평균해서 구한다.


- 좋은 함수가 아니다


- 책 추천(1시간 3분)
> - [가볍게 시작하는 통계학습](http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=81859233)
> - [해들리 위컴의 Advanced R](http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=143252067)
> - [핸즈온 머신러닝](http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=142196914)

## [기타 코드]
- - -
- - -

## [statistical learning theory]

### Training Error
- training set: 모델을 훈련시키는데 사용하는 데이터
- training set을 사용해서 학습 시킨 모델이 우리가 가진 training set을 얼마나 잘 설명하고 있는지를 나타내주는 척도
- cost값은 cost값인데 학습이 끝난 후의 cost값이다.

- model의 Capacity가 높아지면 높아질수록 training error는 무조건 줄어든다.
> - Capacity가 높아진다는 건 모델의 기억력이 늘어난다는 얘기와 같음
> - 모델의 기억력이 늘어난다는 건,
>> - 모델이 단순히 train set을 암기해버리는 사태가 발생한다.
>> - 그게 바로 __overfitting__
>> - 나는 암기하는 게 아니라 실제로 training set을 이해하고, 일반화 시켜주는 작업을 모델에다 시키고 싶은 건데
>> - Capacity가 높으면 단순히 암기하는 결과를 내뱉어주는 암기 머신에 불과하다.
>> - 그 경우가 overfitting이고, 우리는 그 경우를 방지해야 한다. 

### Generalization Error (일반화 오류)
- (1시간 30분)
- 모델이 training set을 가지고 일반화를 했는데, 그 일반화한 결과에 얼마나 오류가 있을 것인가에 대한 양
- 그런데 일반적으로 측정할 수가 없는 양이다.
> - 새로운 데이터가 없다는 뜻, 일반적으로는 측정이 불가능하다.

- Generalization Error 대신에 우리는 Data를 두 개로 쪼갠다.
> - 그래서 G.E를 어떤 식으로 추측(추정)할 것인지 배워볼 것이다.
> - 그전에 이거부터 배울 것이다.


- 우리는 이전에 모델한테 '우리가 가진 training set을 가장 잘 설명하는 함수를 찾아봐'라고 얘기해줬다.
> - training error(cost function)의 최소화
> - 우리가 실제로 하고 싶었던 건, __training error를 낮추는 게 아니라, generalization error를 제일 낮추는 것__이었다.
> - __일반적으로 G.E를 낮추는 건 불가능__하다.
> - GE를 낮추고 싶지만, 그럴 수 없으니까 __대신에 T.E를 낮추는 모델__을 만들기로 함
> - 그렇게 선택한 모형이 과연 G.E도 낮출 것인가?
> - 아니라면 그 둘은 얼마나 차이가 날 것인가?
> - 그걸 이론적으로 분석한 학문이 statistical learning theory다.

### statistical learning theory 부등식
>> - 통계적 학습이론에서 가장 중요한 부등식
>> - 
>> - G.E는 T.E보다 높을까 낮을까?
>>> - training set을 우리가 모델을 training하는데 썼으니까 당연히 모델은 training error는 낮을 것이다(낮추는 방향으로 학습했으니까!)
>>> - 새로운 데이터가 들어오면 에러는 당연히 T.E보다 높을 수밖에 없다.
>>> - 그러면 얼마나 높을 것인가?


### Basic Terminology
- 새로운 데이터를 구하기 어려우니까, 기존의 데이터를 나눠서 훈련으로 모델링을 한 다음에 테스트를 한다!
- 일반적으로 G.E가 T.R보다는 높다.
> - 부등식이 말해주는 건, T.R보다는 커도, T.R + @ 보다는 작다는 것이다.
> - __TR + e >= G.E >= T.R(training error)__
>> -  영희의 키(168) + e >= 철수의 키 >= 영희의 키(168) ??
> - e의 값이 작으면 작을수록 좋겠다.
>> - 영희의 키에 더해주는 'e' 값이 작으면 작을수록 철수의 키를 더 정확하게 말할 수 있다.
> - 우리는 e값이 작아지기를 원한다.
> - 작아야 G.E가 저 범위 안에 들어갈 것이라는 것을 Guarantee할 수 있으니까
> - n은 트레이닝 데이터의 개수
>> - 데이터를 많이 모으면 좋은 것.
>> - __데이터를 많이 모으면 루트 안의 값이 작아진다(값이 낮으면 좋은 것).__
> - D는 model의 capacity임.
>> - D가 커지면 루트 안의 값이 커진다.
>> - __이 값을 줄이기 위해서는 데이터를 많이 모아야 하고, capacity를 줄여야한다.__
- 왜 그런 결과들이 발생했는지 이론적으로 조금이나마 이해해볼 수 있게 되었다.
- 얘는(?) 데이터가 많으니까(n이 크니까), 루트 안의 값이 작아져서 G.E와 T.R 거의 비슷해졌다. 그러니까 overfitting이 발생하지 않았다고 해석할 수 있게 되었다.

### structural risk minimization & Empirical Risk Minimization
- training error를 줄이는 게 아니라, e값을 줄여버리면 되지 않겠느냐?
> - __structural risk minimization__ (구조적 위협을 최소화하는 것)
- 계산하는 게 너무 어렵기 때문에, T.R를 최대한 줄여라. 그러면 그 모델의 G.E도 작을 것이다.
> - T.R를 줄이는 방향으로 학습시키는 방법 : __Empirical Risk Minimization__ (경험적인)
>> - 관측할 데이터를 가지고 loss(cost)를 최소화 시켜라.
>> - 그렇게 만든 모델이 새로운 데이터에 대한 예측력도 나쁘지 않을 것이라고 생각하고
>> - 학습을 시킴
>> - 머신러닝에서 무조건 이 방법을 사용함


- 1시간 40분!
- 일반화된 
- 암기된 
- 최적의 optimal capacity를 찾자!
- - -
- - -
- - -
# 2-3. K-Fold Cross Validation
## 대부분의 사람들이 헷갈리는 개념들
- 768개의 data set
> - 일반적으로 70%는 training set, 30%는 test set로 나눈다.
> - training set의 모형이 데이터를 통해 학습하고, 학습한 모형(모델)이 나온다.
> - test set에는 input도 있고 output도 있다.
>> - test set에 있는 input을 기계(모형)에 넣어서 output을 예측하게 한 다음에
>> - 두 개가 얼마나 차이가 나는지 본다.
>>> - maen((y - y_hat)^2)
>>> - y는 실제값, y_hat은 예측값?
> - mean(y - y_hat_n)^2
>> - 거기에서 나온 게 test error
>>> - test error이 가장 낮은 모형을 선택해야 한다?
>>> - training error는 높은 차수의 모델이 가장 낮을 것이지만
>>> - test error는 당연히 그렇지 않다.
>>> - 그래서 우리는 test set에 대한 error가 가장 낮아지는 모형을 선택할 것이다?
>>> - 틀린 이야기!

(1시간 48분 50초)
- 새로운 데이터를 얼마나 잘 예측하는지 보겠다는 것
- G.E에 대한 추정치가 T.R이다.
> - T.R는 G.E를 최대한 정확하게 추정하기 위해서 필요한 세트다.
> - T.R와 G.E는 최대한 비슷해야 한다.

## [실습 2](http://localhost:8888/notebooks/Downloads/dev/study/R_ML/Class%202/ML_2_script.ipynb)




[실습] (1시간 50분 30초)
- 샘플의 수가 적으면 ... (2시간 12분 20초)
> - 트레이닝 셋을 임의로 k등분함.
> - 이 알고리즘은 cross vailation

- loocv는 k-fold CV보다 안 좋다.

- 논리의 흐름 (2시간 21분 53초)
> - test set의 역할은 G.E를 최대한 정확하게 추정하는데 본질이 있다.
>> - G.E: 모델을 트레이닝하는데 사용하지 않은 데이터의 에러
>> - 각각의 모델의 test를 확인하고, 가장 작은 걸 선택한다.
>> - 세 개의 모델 중에서 마지막 모델을 선택했다는 건, 직접적으로 트레이닝 셋을 사용한 것이다.
>> - 세 개의 모델 중에 하나를 선택하는 과정 중에 우리는 무엇을 사용했는가?
>>> - test set 사용함. 10차 함수는 간접적으로 test set본 것과 다름 없다.
>>> - test set은 모델이 전혀 본적이 없는 데이터를 추정하는데 쓰이는 게 본질
>>> - 허나 봤음... test set이 오염 됐다.
>>> - 다른 데이터가 있어야 한다. 그래서 데이터가 2등분이 아니라 3등분을 해야한다.
> - test set과 또 다른 데이터 셋이 필요하다! (2시간 26분)
>> - Training set // Validation Set // Test Set
>>> - Train Set: 모델 훈련, Valid set: 모델을 추정하고 선택 
>>> - 최종적으로 GE를 정확하게 추정되는데 사용하는 데이터가 Test Set
>>> - 연구 윤리: Test Set을 최종적으로 써야 한다.
- [ibm 왓슨 아주대병원](http://news.chosun.com/site/data/html_dir/2018/01/10/2018011002828.html) (2시간 29분)
> - 통계적으로 bias 되는 현상이 발생한다.
> - test set은 끝까지, 최후까지 나눠둬야 한다.

- 질문 (2시간 31분)
- 데이터를 나누는 비율
> - (중규모) 6:2:2 / (대규모) 9:1 or 8:2

